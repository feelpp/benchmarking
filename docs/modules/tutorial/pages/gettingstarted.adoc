= Getting started

== Installation

1. Clone the Repository
[source,cmd]
----
git clone https://github.com/feelpp/benchmarking.git
----

2. Use a python virtual environment [Optional]
[source,cmd]
----
python3 -m venv .venv
source .venv/bin/activate
----

3. Build the project
[source,cmd]
----
pip3 wheel --no-deps --wheel-dir dist .
----

4. Install requirements

This will install necessary dependencies as well as the built project from the previous step.
[source,cmd]
----
python3 -m pip install -r requirements.txt
----

== Quickstart

The framwork includes a sample C++/MPI application that can be used to get familiar with the framework's core concepts. It can be found under _tests/data/parallelSum.cpp_.

This Feel++ Benchmarking "Hello World" application will compute the sum of an array distributed across multiple MPI processes. Each process will compute a partial sum, and then it will be summed to get the total sum.

Additionally, the app will measure the time taken to perform the partial sum, and will save it under a _scalability.json_ file.

The executable is already provided as _tests/data/parallelSum_. You can update it and recompile it for a specific config as needed.
[source,cmd]
----
mpic++ -std=c++17 -o test/data/parallelSum test/data/parallelSum.cpp
----

// Local configuration files can be found under _config/test_parallelSum/_. They are explained more in detail in the following configuration section.
// This folder cantins a _parallelSum.json_ file holding are benchmark related configurations. That is, instructions on what the benchmark should be. It also contains a `plots.json` file holding descriptions on the figures that should be generated for this benchmark.
// Additionally, machine specific configurations are found under _config/machines/_. These files contain general information on the environments to run the tests on, including containers, special options and base directories for inputs and outputs of the applications.
// It is important to know that these configuration files are user dependent, and will most certainly vary depending on the resource you will execute benchmarks on. For a simple example, the _local.json_ file can be used for running the application on a personal Linux or MacOS computer.

Configuration files might require some changes for specific configurations depending on the system you are running the framework.

Finally, to benchmark the test application, generate the reports and plot the figures, run
[source,cmd]
----
execute-benchmark --machine-config config/machines/local.json \
                    --benchmark-config config/test_parallelSum/parallelSum.json \
                    --plots-config config/test_parallelSum/plots.json \
                    --website
----

The `--website` option will start an http-server on localhost, so the website can be visualized, check the console for more information.


