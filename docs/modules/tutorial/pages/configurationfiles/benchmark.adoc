= Benchmark configuration
:page-plotly: true
:page-jupyter: true

Configuring a benchmark can be quite extensive, as this framework focuses on flexibility. For this, the documentation will be divided in main sections.

The benchmark configuration file describes precisely how the benchmarking should be done, for example, specifying where the executable is located, the options to pass to the application, and how the tests will be parametrized.

The base of the configuration file is shown below.
[source,json]
----
{
    "executable": "",
    "use_case_name": "",
    "timeout":"",
    "env_variables":{},
    "options": [],
    "resources":{},
    "platforms":{},
    "additional_files":{},
    "scalability":{},
    "sanity":{},
    "parameters":{},
}
----

[TIP]
====
Users can add any field used for refactoring. For example, one can do the following.

[source,json]
----
"output_directory":"/data/outputs" // This is a custom field
"options":["--output {{output_directory}}"]
----
====

include::benchmark/base.adoc[leveloffset=+1]

include::benchmark/resources.adoc[leveloffset=+1]

include::benchmark/platforms.adoc[leveloffset=+1]

include::benchmark/scalability.adoc[leveloffset=+1]

include::benchmark/additionalFiles.adoc[leveloffset=+1]

include::benchmark/sanity.adoc[leveloffset=+1]

include::benchmark/parameters.adoc[leveloffset=+1]

[TIP]
====
More advanced features are available (xref:tutorial:advancedConfiguration.adoc[Advanced Configuration])such as:

- Downloading remote data
- Copying input file between disks
- Pruning the parameter space
- Specifying custom performance variables
====