name: Benchmark

on:
    repository_dispatch:
        types: [execute-benchmark]

    workflow_dispatch:
        inputs:
            machines_config:
                description: 'Machine related configurations'
                required: True
            benchmark_config:
                description: 'Applcation related configuration'
                required: True
            plots_config:
                description: 'Plots related configuration'
                required: True
            girder_folder_id:
                description: 'ID of the folder to upload to'
                required: True

jobs:

    build_wheel:
        runs-on: self-ubuntu-22.04
        name: Build wheel package
        if: "!contains(github.event.head_commit.message, 'code skip')"
        steps:
        -   uses: actions/checkout@v4
        -   name: Build Ktirio Cases wheel
            run: |
                npm install
                npx downdoc README.adoc
                pip3 wheel --no-deps --wheel-dir dist .
            env:
                CXX: clang++
        -   name: Upload Artifact
            uses: actions/upload-artifact@v4
            with:
                name: wheel-artifacts
                path: dist/*.whl

    factory:
        name: HPC Systems Factory
        needs: build_wheel
        runs-on: self-ubuntu-22.04
        outputs:
            matrix: ${{ steps.hpc-systems.outputs.matrix }}
            executable_name: ${{ steps.gather_arguments.outputs.executable_name }}
            use_case: ${{ steps.gather_arguments.outputs.use_case }}
        steps:
        -   name: Download wheel
            uses: actions/download-artifact@v4
            with:
                name: wheel-artifacts
                path: dist
        -   name: Create Virtual Environment
            run: |
                python3 -m venv .venv
                source .venv/bin/activate
                pip3 install -r requirements.txt
        -   name: Download machines configuration
            run: |
                source .venv/bin/activate

                if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
                    machine_cfg_id=${{ github.event.inputs.machines_config }};
                elif [[ "${{ github.event_name}}" == "repository_dispatch" ]]; then
                    machine_cfg_id=${{ github.event.client_payload.machines_config }};
                fi
                girder-download -gid $machine_cfg_id -o ./tmp/ -fn "machines_config.json"
            env:
                GIRDER_API_KEY: ${{secrets.GIRDER}}
        -   name: Donwload benchmark configuration
            run: |
                source .venv/bin/activate

                if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
                    bench_cfg_id=${{ github.event.inputs.benchmark_config }};
                    plots_cfg_id=${{ github.event.inputs.plots_config }};
                elif [[ "${{ github.event_name}}" == "repository_dispatch" ]]; then
                    bench_cfg_id=${{ github.event.client_payload.benchmark_config }};
                    plots_cfg_id=${{ github.event.client_payload.plots_config }};
                fi
                girder-download -gid $bench_cfg_id -o ./tmp/ -fn "benchmark_config.json"
                girder-download -gid $plots_cfg_id -o ./tmp/ -fn "plots.json"
            env:
                GIRDER_API_KEY: ${{secrets.GIRDER}}
        -   id: hpc-systems
            name: Set HPC systems matrix
            run: |
                source .venv/bin/activate
                matrix=$(hpc-dispatch -mcp ./tmp/machines_config.json -mod ./tmp/machines/ -bcp ./tmp/benchmark_config.json -pcp ./tmp/plots.json -mv ./tmp/results)
                echo $matrix
                echo "matrix={ include : $matrix }" >> $GITHUB_OUTPUT
        -   name: pull_images
            run: |
                source .venv/bin/activate
                #ONLY IF NEEDED
        -   name: Gather arguments
            id: gather_arguments
            run: |
                echo "executable_name=$(jq -r '.executable' ./tmp/benchmark_config.json)" >> $GITHUB_OUTPUT
                echo "use_case=$(jq -r '.use_case_name' ./tmp/benchmark_config.json)" >> $GITHUB_OUTPUT
        -   name: Upload configs
            uses: actions/upload-artifact@v4
            with:
                name: config-artifacts
                path: |
                    ./tmp/benchmark_config.json
                    ./tmp/plots.json
                    ./tmp/machines/

    benchmark:
        needs: factory
        strategy:
            fail-fast: false
            matrix: ${{ fromJson(needs.factory.outputs.matrix) }}
        runs-on: ${{matrix.runner}}
        timeout-minutes: 7200
        name: ${{matrix.machine}}
        steps:
        -   uses: actions/checkout@v4
        -   name: Download wheel
            uses: actions/download-artifact@v4
            with:
                name: wheel-artifacts
                path: dist
        -   name: Download configs
            uses: actions/download-artifact@v4
            with:
                name: config-artifacts
                path: ./tmp/
        -   name: Create Virtual Environment
            run: |
                python${{matrix.python_version}} -m venv .venv
                source .venv/bin/activate
                python${{matrix.python_version}} -m pip install --upgrade pip
                python${{matrix.python_version}} -m pip install -r requirements.txt

        -   name: Execute benchmarks (cli)
            run: |
                source .venv/bin/activate
                $${{matrix.submit_command}}

        -   name: Upload reframe report
            uses: actions/upload-artifact@v4
            with:
                name: benchmark-results
                path: ./tmp/results/

    results:
        runs-on: self-ubuntu-22.04
        needs: [benchmark,factory]
        name: Handle results and trigger rendering

        steps:
        -   uses: actions/checkout@v4
        -   name: Download results
            uses: actions/download-artifact@v4
            with:
                name: benchmark-results
                path: ./tmp/results/
        -   name: Create Virtual Environment
            run: |
                python3 -m venv .venv
                source .venv/bin/activate
                pip3 install -r requirements.txt
        -   name: Upload to girder
            run: |
                source .venv/bin/activate
                new_foldername=./tmp/$(date +"%Y_%m_%dT%H_%M_%S")
                mv ./tmp/results/ $new_foldername
                if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
                    girder_upload_id=${{ github.event.inputs.girder_folder_id }};
                elif [[ "${{ github.event_name}}" == "repository_dispatch" ]]; then
                    girder_upload_id=${{ github.event.client_payload.girder_folder_id }};
                fi
                girder-upload --directory $new_foldername --girder_id $girder_upload_id
                rm -r $new_foldername
            env:
                GIRDER_API_KEY: ${{ secrets.GIRDER }}
        -   name:  Reset reports
            run: |
                rm -r ./docs/modules/ROOT/pages/applications/
                rm -r ./docs/modules/ROOT/pages/machines/
                rm -r ./docs/modules/ROOT/pages/reports/
                rm -r ./docs/modules/ROOT/pages/use_cases/
                rm -r ./reports/
        -   name: Render reports
            run: |
                source .venv/bin/activate
                render-benchmarks
            env:
                GIRDER_API_KEY: ${{ secrets.GIRDER }}

        -   name: Create Pull Request
            uses: peter-evans/create-pull-request@v7
            with:
                title: "Add benchmark for ${{ needs.factory.outputs.executable_name }} - ${{ needs.factory.outputs.use_case }}"
                body: |
                    Auto-generated by [create-pull-request][1]
                    [1]: https://github.com/peter-evans/create-pull-request
                reviewers: JavierCladellas
            env:
                GITHUB_TOKEN: ${{ secrets.CR_PAT }}